# -*- coding: utf-8 -*-
"""resnet_cifar100v2_0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ThDhRMA2X4CtO3u2L8MDvaPWapv6V8VN
"""

import os
import torch
import torch.nn as nn
import torchvision
import torch.utils.data
import torch.optim
import torchvision.transforms as transforms
import numpy as np
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#resnet module
def conv3x3(in_channels, out_channels, stride = 1):
    return nn.Conv2d(in_channels = in_channels, out_channels = out_channels,
                   kernel_size =3, stride = stride, padding = 1, bias = False)

class BasicBlock(nn.Module):
      
  
  def __init__(self, in_channels, out_channels, stride = 1, downsample = None):
    super(BasicBlock, self).__init__()
    
    self.conv1 = conv3x3(in_channels, out_channels, stride)
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.relu = nn.ReLU(inplace = True)
    
    self.conv2 =conv3x3(out_channels, out_channels, stride = 1)
    self.bn2 = nn.BatchNorm2d(out_channels)
    self.relu = nn.ReLU(inplace = True)
    
    self.downsample= downsample
    self.stride = stride
    
  def forward(self,x):
    residual = x
    output = self.conv1(x)
    output = self.bn1(output)
    output = self.relu(output)
    
    output = self.conv2(output)
    output = self.bn2(output)
    
    
    if self.downsample is not None:
      residual = self.downsample(x)
    output += residual
    output = self.relu(output)
    
    return output

class ResNet(nn.Module):
  
  
  def __init__(self, block, dup_blocks, num_classes):
    
    super(ResNet, self).__init__()
    
    self.conv1 = conv3x3(in_channels = 3, out_channels = 32)
    self.in_channels = 32
    self.bn1 = nn.BatchNorm2d(num_features = 32)
    self.relu = nn.ReLU(inplace = True)
    self.dropout = nn.Dropout2d(p = 0.1)
    
    self.conv2_x = self._make_layer(block, dup_blocks[0], out_channels = 32)
    self.conv3_x = self._make_layer(block, dup_blocks[1], out_channels = 64, stride = 2)
    self.conv4_x = self._make_layer(block, dup_blocks[2], out_channels = 128, stride = 2)
    self.conv5_x = self._make_layer(block, dup_blocks[3], out_channels = 256, stride = 2)
    
    self.maxpool = nn.MaxPool2d(kernel_size = 4, stride = 1)
    self.fc_layer = nn.Linear(256, num_classes)
    
        
    
  def _make_layer(self, block, dup_blocks, out_channels, stride = 1):
    
    downsample = None
    if (stride != 1) or (self.in_channels != out_channels):
          downsample = nn.Sequential(
              conv3x3(self.in_channels, out_channels, stride = stride),
              nn.BatchNorm2d(num_features = out_channels))
          
    layers = []
    layers.append(block(self.in_channels, out_channels, stride, downsample))
    self.in_channels = out_channels
    
    for _ in range(1, dup_blocks):
      layers.append(block(self.in_channels, out_channels))

    return nn.Sequential(*layers)
      
  def forward(self,x):
    
    output = self.conv1(x)
    output = self.bn1(output)
    output = self.relu(output)
    output = self.dropout(output)
    
    output = self.conv2_x(output)
    output = self.conv3_x(output)
    output = self.conv4_x(output)
    output = self.conv5_x(output)
    
    output = self.maxpool(output)
    output = output.view(output.size(0),-1)
    output = self.fc_layer(output)
    
    return output

#data loader

def data_loader(dataroot, batch_size_train, batch_size_test):
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    trainset = torchvision.datasets.CIFAR100(root=dataroot,
                                             train=True,
                                             download=True,
                                             transform=transform_train)
    trainloader = torch.utils.data.DataLoader(
        trainset, batch_size=batch_size_train, shuffle=True, num_workers=4)

    testset = torchvision.datasets.CIFAR100(root=dataroot,
                                            train=False,
                                            download=True,
                                            transform=transform_test)
    testloader = torch.utils.data.DataLoader(
        testset, batch_size=batch_size_test, shuffle=False, num_workers=4)

    return trainloader, testloader

#training modules


def calculate_accuracy(net, loader):
    correct = 0
    total = 0
    net.eval()
    with torch.no_grad():
      for (images, labels) in loader:
          device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
          images = images.to(device)
          labels = labels.to(device)

          outputs = net(images)
          _, predicted = torch.max(outputs.data,1)
          total += labels.size(0)
          correct += (predicted==labels).sum()
    
    net.train()
    return (100.0*correct/total)

  
def train(net, criterion, optimizer, trainloader, testloader, epochs):
    print("Training in progress...")
    log_avg_loss = []
    log_epoch = []
    log_train_accuracy = []
    log_test_accuracy = []
    
    for epoch in range (epochs):
        start = time.time()
        net.train()
        running_loss = 0.0
        
        batch_start = time.time() #Batch Start timer initialized
        for batch_indx, (images, labels) in enumerate(trainloader):
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            images = images.to(device)
            labels = labels.to(device)

            outputs = net(images)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            if(batch_indx+1)% 100 == 0:
              batch_end = time.time() #Batch end timer
              print("[{0}/{1}]: Average Loss = {2} | Time Elapsed = {3} min".format(batch_indx+1, len(trainloader), running_loss/(batch_indx+1), 
                                                                                   (batch_end - batch_start)/60))
              batch_start = time.time()#restting Batch timer
                                 
        scheduler.step(loss)
        avg_loss= running_loss/len(trainloader)
        train_accuracy = calculate_accuracy(net, trainloader)
        test_accuracy = calculate_accuracy(net, testloader)
        
        log_epoch.append(epoch+1)
        log_avg_loss.append(avg_loss)
        log_train_accuracy.append(train_accuracy)
        log_test_accuracy.append(test_accuracy)
        
        end = time.time()
        print ("Epoch: {0} || Loss: {1} || Training Accuracy: {2}%  ||  Testing Accuracy: {3}% || Time Elapsed: {4}".format(
            epoch+1, avg_loss, train_accuracy, test_accuracy, (end-start)/60))
        
        #FOR DEBUGGING TESTING
        print("This is for testing purposes\n\n",
              "epoch:", log_epoch,
              "\navg_loss:", log_avg_loss, 
              "\nlog_train_accuracy:", log_train_accuracy, 
              "\nlog_test_accuracy:", log_test_accuracy)
        
        
        
            
    print("Training Completed!")
    return(log_epoch, log_avg_loss, log_train_accuracy, log_test_accuracy)

lr = 0.01
momentum= 0.9
weight_decay = 0
step_size = 10
gamma = 0.1
epochs = 20
num_classes = 100
dup_blocks = [2,4,4,2]
batch_size = 64
dataroot = './data'


train_loader, test_loader = data_loader(dataroot, batch_size, batch_size)

net = ResNet(BasicBlock, dup_blocks, num_classes)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
net = net.to(device)

# Loss function, optimizer and scheduler
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(),
                             lr=lr,
                             momentum=momentum,
                             weight_decay=weight_decay)

#optimizer = torch.optim.Adam(net.parameters(), 
#                       lr = lr, 
#                       weight_decay=1e-5)
#scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
#                                       step_size = step_size,
#                                       gamma = gamma)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience= 2, verbose=True)



# training
epoch, loss, train_ac, test_ac = train(net, criterion, optimizer, train_loader, test_loader, epochs)

df = pd.DataFrame()
df['epoch'] = epoch
df['loss'] = loss
df['train_ac'] = train_ac
df['test_ac'] = test_ac

for i in ['loss', 'train_ac', 'test_ac']:
  plt.plot('epoch', i, data = df, linestyle = '-', marker = 'o', label = i)
  
plt.legend(shadow=True, fontsize='x-large')
plt.show()

df['test_ac']

