# -*- coding: utf-8 -*-
"""sync_sgd_cifar100.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DKQikyy8yakQBjZsO45R2u8bjMLfAyTP
"""

import os
import torch
import torch.nn as nn
import torchvision
import torch.utils.data
import torch.optim
import torchvision.transforms as transforms
from torch.autograd import Variable


import numpy as np
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


import torch.distributed as dist
import os
import subprocess
from mpi4py import MPI

cmd = "/sbin/ifconfig"
out, err = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,
stderr=subprocess.PIPE).communicate()
ip = str(out).split("inet addr:")[1].split()[0]
name = MPI.Get_processor_name()
comm = MPI.COMM_WORLD
rank = comm.Get_rank()
num_nodes = int(comm.Get_size())
ip = comm.gather(ip)
if rank != 0:
  ip = None
ip = comm.bcast(ip, root=0)
os.environ['MASTER_ADDR'] = ip[0]
os.environ['MASTER_PORT'] = '2222'
backend = 'mpi'
dist.init_process_group(backend, rank=rank, world_size=num_nodes)
dtype = torch.FloatTensor



#data loader

def data_loader(dataroot, batch_size_train, batch_size_test):
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    trainset = torchvision.datasets.CIFAR100(root=dataroot,
                                             train=True,
                                             download=True,
                                             transform=transform_train)
    trainloader = torch.utils.data.DataLoader(
        trainset, batch_size=batch_size_train, shuffle=True, num_workers=0)

    testset = torchvision.datasets.CIFAR100(root=dataroot,
                                            train=False,
                                            download=True,
                                            transform=transform_test)
    testloader = torch.utils.data.DataLoader(
        testset, batch_size=batch_size_test, shuffle=False, num_workers=0)

    return trainloader, testloader

#training modules


def calculate_accuracy(net, loader):
    correct = 0
    total = 0
    net.eval()

    for (images, labels) in loader:
      images = Variable(images).cuda()
      labels = Variable(labels).cuda()
      
      outputs = net(images)
      _, predicted = outputs.max(1)
      total += labels.size(0)
      correct += float(predicted.eq(labels).sum())
      
    net.train()
    return (100.0*correct/total)

  
def train(net, criterion, optimizer, trainloader, testloader, epochs):
    print("Training in progress...")
    log_avg_loss = []
    log_epoch = []
    log_train_accuracy = []
    log_test_accuracy = []
    net.train()
    
    for epoch in range (epochs):
        start = time.time()
        net.train()
        running_loss = 0.0
        
        batch_start = time.time() #Batch Start timer initialized
        for batch_indx, (images, labels) in enumerate(trainloader):
          images = Variable(images).cuda()
          labels = Variable(labels).cuda()
          
          optimizer.zero_grad()
          outputs = net(images)
          loss = criterion(outputs, labels)
          loss.backward()
          
          for param in net.parameters():
            tensor0 = param.grad.data.cpu()
            dist.all_reduce(tensor0, op=dist.reduce_op.SUM)
            tensor0 /= float(num_nodes)
            param.grad.data = tensor0.cuda()
          
          optimizer.step()
          running_loss += float(loss)
          
          if(batch_indx+1)% 100 == 0:
            batch_end = time.time() #Batch end timer
            print("[{0}/{1}]: Average Loss = {2} | Time Elapsed = {3} min".format(batch_indx+1, len(trainloader), running_loss/(batch_indx+1), 
                                                                                   (batch_end - batch_start)/60))
            batch_start = time.time()#restting Batch timer
            
          
        scheduler.step()
        avg_loss= running_loss/len(trainloader)
        train_accuracy = calculate_accuracy(net, trainloader)
        test_accuracy = calculate_accuracy(net, testloader)

        log_epoch.append(epoch+1)
        log_avg_loss.append(avg_loss)
        log_train_accuracy.append(train_accuracy)
        log_test_accuracy.append(test_accuracy)
        
        end = time.time()
        print ("Epoch: {0} || Loss: {1} || Training Accuracy: {2}%  ||  Testing Accuracy: {3}% || Time Elapsed: {4}".format(
            epoch+1, avg_loss, train_accuracy, test_accuracy, (end-start)/60))
        
        #FOR DEBUGGING TESTING
        print("This is for testing purposes\n\n",
              "epoch:", log_epoch,
              "\navg_loss:", log_avg_loss, 
              "\nlog_train_accuracy:", log_train_accuracy, 
              "\nlog_test_accuracy:", log_test_accuracy)
        
        
        
            
    print("Training Completed!")
    return(log_epoch, log_avg_loss, log_train_accuracy, log_test_accuracy)
    
    

#resnet module

def conv3x3(in_channels, out_channels, stride = 1):
    return nn.Conv2d(in_channels = in_channels, out_channels = out_channels,
                   kernel_size =3, stride = stride, padding = 1, bias = False)

class BasicBlock(nn.Module):
      
  
  def __init__(self, in_channels, out_channels, stride = 1, downsample = None):
    super(BasicBlock, self).__init__()
    
    self.conv1 = conv3x3(in_channels, out_channels, stride)
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.relu = nn.ReLU(inplace = True)
    
    self.conv2 =conv3x3(out_channels, out_channels, stride = 1)
    self.bn2 = nn.BatchNorm2d(out_channels)
    self.relu = nn.ReLU(inplace = True)
    
    self.downsample= downsample
    self.stride = stride
    
  def forward(self,x):
    residual = x
    output = self.conv1(x)
    output = self.bn1(output)
    output = self.relu(output)
    
    output = self.conv2(output)
    output = self.bn2(output)
    
    
    if self.downsample is not None:
      residual = self.downsample(x)
    output += residual
    output = self.relu(output)
    
    return output

class ResNet(nn.Module):
  
  
  def __init__(self, block, dup_blocks, num_classes):
    
    super(ResNet, self).__init__()
    
    self.conv1 = conv3x3(in_channels = 3, out_channels = 32)
    self.in_channels = 32
    self.bn1 = nn.BatchNorm2d(num_features = 32)
    self.relu = nn.ReLU(inplace = True)
    self.dropout = nn.Dropout2d(p = 0.1)
    
    self.conv2_x = self._make_layer(block, dup_blocks[0], out_channels = 32)
    self.conv3_x = self._make_layer(block, dup_blocks[1], out_channels = 64, stride = 2)
    self.conv4_x = self._make_layer(block, dup_blocks[2], out_channels = 128, stride = 2)
    self.conv5_x = self._make_layer(block, dup_blocks[3], out_channels = 256, stride = 2)
    
    self.maxpool = nn.MaxPool2d(kernel_size = 4, stride = 1)
    self.fc_layer = nn.Linear(256, num_classes)
    
        
    
  def _make_layer(self, block, dup_blocks, out_channels, stride = 1):
    
    downsample = None
    if (stride != 1) or (self.in_channels != out_channels):
          downsample = nn.Sequential(
              conv3x3(self.in_channels, out_channels, stride = stride),
              nn.BatchNorm2d(num_features = out_channels))
          
    layers = []
    layers.append(block(self.in_channels, out_channels, stride, downsample))
    self.in_channels = out_channels
    
    for _ in range(1, dup_blocks):
      layers.append(block(self.in_channels, out_channels))

    return nn.Sequential(*layers)
      
  def forward(self,x):
    
    output = self.conv1(x)
    output = self.bn1(output)
    output = self.relu(output)
    output = self.dropout(output)
    
    output = self.conv2_x(output)
    output = self.conv3_x(output)
    output = self.conv4_x(output)
    output = self.conv5_x(output)
    
    output = self.maxpool(output)
    output = output.view(output.size(0),-1)
    output = self.fc_layer(output)
    
    return output

lr = 0.01
momentum= 0.9
weight_decay = 1e-5
step_size = 10
gamma = 0.1
epochs = 50
num_classes = 100
dup_blocks = [2,4,4,2]
batch_size = 32
dataroot = './data'


train_loader, test_loader = data_loader(dataroot, batch_size, batch_size)

net = ResNet(BasicBlock, dup_blocks, num_classes)
net = net.cuda()

# Loss function, optimizer and scheduler
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(),
                             lr=lr,
                             momentum=momentum,
                             weight_decay=weight_decay)

#optimizer = torch.optim.Adam(net.parameters(), 
#                       lr = lr, 
#                       weight_decay=1e-5)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                       step_size = step_size,
                                       gamma = gamma)

#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience= 2, verbose=True)



# training
epoch, loss, train_ac, test_ac = train(net, criterion, optimizer, train_loader, test_loader, epochs)

df = pd.DataFrame()
df['epoch'] = epoch
df['loss'] = loss
df['train_ac'] = train_ac
df['test_ac'] = test_ac

for i in ['loss', 'train_ac', 'test_ac']:
  plt.plot('epoch', i, data = df, linestyle = '-', marker = 'o', label = i)
  
plt.legend(shadow=True, fontsize='x-large')
plt.show()